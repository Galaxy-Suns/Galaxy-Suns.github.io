# 反向传播

调用`model.fit`后`TensorFlow`自动使用反向传播计算导数，以便梯度下降得到合适的参数

*导数是如何计算的？*

## 导数

### 导数的认识

这是一个简化的成本函数

![](img/231c9846.png)

我们设置`w`为3，那么`J=9`

如果我们给`w`增加一个很小值`ε=0.001`，`J(w)`如何变化？

![](img/95177bdb.png)

大约如下

![](img/f515c510.png)

如果ε足够小，那么这个结论会愈发准确

实际上，**J(w)的导数为6**

* 通过导数的极限定义就可以解释这个现象

而回到梯度下降的例子

![](img/d0c2e38a.png)

* 在学习率一定时，如果导数项很小，那么w不会更新很多，如果导数项很大，那么w更新一大截。
  * 结合我们上面的发现，如果导数很小，意味着更改`w`，不会太多影响`J`

这是更多**形象化导数定义的例子**

而我们还可以观察到

* **导数是原函数的切线斜率**
* J关于w导数的具体值取决于w

![](img/e8215221.png)

我们通过试验或者微积分的知识可以得到导函数

### python求导

python中的`symPy`可以计算导数

```py
>>> import sympy
>>> J, w = sympy.symbols('J, w')
>>> J
J
>>> J = w ** 2
>>> J
w**2
>>> sympy.diff(J, w) # 求 J 关于 w 的导数
2*w
>>> dJ_dw = sympy.diff(J, w)
>>> dJ_dw.subs([(w, 2)])# 根据导函数计算具体的导数值 实际是将w=2插入这个符号表达式
4
```

![](img/bf673b6c.png)

## 神经网络中的导数 计算图

**计算图**是计算神经网络的导数的关键，也是如`TensorFlow`这种框架自动计算导数的工具

对于一层神经网络（一个线性激活神经元） 输入矢量x,输出激活值a

`a=Wx+b=g(z)=z`

其成本函数即为`1/2 * (a-y)^2` (由于我们只用一个训练示例)

这是网络的训练集和当前参数

![](img/b5fe3923.png)

计算图计算成本函数

下面是`J(w, b)`的计算

1. 首先计算`c=wx=-4`
2. 计算 `a=c+b=4`
3. 计算 `d=a-y`
